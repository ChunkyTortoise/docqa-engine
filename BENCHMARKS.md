# DocQA Engine Retrieval Benchmarks

Evaluated on 15 queries against 3 demo documents (Python guide, ML textbook, Startup playbook).

| Method | Precision@5 | Hit Rate@5 | Avg Latency |
|--------|---------------|---------------|---------------|
| BM25 | 29.3% | 86.7% | 0.2ms |
| Dense (TF-IDF) | 28.0% | 86.7% | 0.2ms |
| Hybrid (BM25 + Dense + RRF) | 29.3% | 86.7% | 0.4ms |

**Methodology**: Keyword-based relevance judgment. A result is relevant if it contains at least one expected keyword. Precision@5 = relevant results in top 5 / 5. Hit Rate = queries with at least 1 relevant result in top 5.

**Embedder**: TF-IDF (5000 features, scikit-learn). No external API calls required.

Generated by `benchmarks/run_benchmarks.py`.
